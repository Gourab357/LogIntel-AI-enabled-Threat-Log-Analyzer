# -*- coding: utf-8 -*-
"""Knowledge graph embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfocMZFh2WlJbf3rU_WI84DgOwcgQQY_
"""

!pip install --upgrade numpy pandas networkx scikit-learn plotly gensim pykeen torch==2.0.1+cpu torchvision==0.15.2+cpu --extra-index-url https://download.pytorch.org/whl/cpu

# ===== Cell 2: Imports & Config =====
import os
import pandas as pd
import numpy as np
import networkx as nx
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
import plotly.express as px
import plotly.graph_objects as go
from pykeen.pipeline import pipeline
from pykeen.triples import TriplesFactory

# Path to CSV (update as needed)
CSV_PATH = '/content/drive/MyDrive/LLM4Sec/Week4/BGL_log_train.csv'
SAMPLE_LIMIT = 500  # max nodes for 3D visualization

# ===== Cell 3: Drive Mount & Data Loading =====
from google.colab import drive

drive.mount('/content/drive', force_remount=True)

if not os.path.isfile(CSV_PATH):
    raise FileNotFoundError(f"Log CSV not found at {CSV_PATH}")

df = pd.read_csv(CSV_PATH)
# Keep only essential columns
df = df.dropna(subset=['Location1', 'Severity', 'Message'])
print(f"Loaded {len(df)} log entries.")

# ===== Cell 4: Build Knowledge Graph =====
def build_knowledge_graph(df: pd.DataFrame) -> nx.MultiDiGraph:
    G = nx.MultiDiGraph()
    for idx, row in df.iterrows():
        event_id = f"event_{idx}"
        G.add_node(event_id, type='event', description=row['Message'])
        G.add_node(row['Location1'], type='component')
        G.add_node(row['Severity'], type='severity')
        G.add_edge(row['Location1'], event_id, relation='emits_event')
        G.add_edge(event_id, row['Severity'], relation='has_severity')
    return G

G = build_knowledge_graph(df)
print(f"KG built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")

# ===== Cell 5: Extract Triples & Split =====
triples = [(u, data['relation'], v) for u, v, data in G.edges(data=True)]
train_triples, test_triples = train_test_split(triples, test_size=0.2, random_state=42)
train_tf = TriplesFactory.from_labeled_triples(np.array(train_triples))
test_tf = TriplesFactory.from_labeled_triples(np.array(test_triples))
print(f"Train/test split: {len(train_triples)} / {len(test_triples)} triples.")

# ===== Cell 6: Train Structural Embeddings (TransE) =====nprint("Training TransE embedding...")
result = pipeline(
    training=train_tf,
    testing=test_tf,
    model='TransE',
    model_kwargs={'embedding_dim': 50},
    training_kwargs={'num_epochs': 20, 'batch_size': 512},
    random_seed=42,
)
model = result.model
entity_ids = result.training.triples_factory.entity_to_id
entity_embs = model.entity_representations[0](indices=None).detach().cpu().numpy()
struct_emb = {ent: entity_embs[idx] for ent, idx in entity_ids.items()}

# ===== Cell 7: Train Textual Embeddings (Word2Vec) =====
descriptions = [nx.get_node_attributes(G, 'description').get(n, '') for n in G.nodes]
sentences = [simple_preprocess(desc) for desc in descriptions if desc]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)

text_emb = {}
for n in G.nodes:
    desc = G.nodes[n].get('description', '')
    tokens = simple_preprocess(desc)
    vecs = [w2v_model.wv[w] for w in tokens if w in w2v_model.wv]
    text_emb[n] = np.mean(vecs, axis=0) if vecs else np.zeros(100)

# ===== Cell 8: Fuse Embeddings & 2D Visualization =====nfused_emb = {
    n: np.concatenate([struct_emb.get(n, np.zeros(50)), text_emb[n]])
    for n in G.nodes
}
X = np.vstack(list(fused_emb.values()))
labels = list(fused_emb.keys())
# 2D t-SNE
proj2d = TSNE(n_components=2, random_state=42, init='pca').fit_transform(X)
vis_df = pd.DataFrame({'x': proj2d[:,0], 'y': proj2d[:,1]})
vis_df['node'] = labels
vis_df['type'] = [G.nodes[n]['type'] for n in labels]
fig2d = px.scatter(vis_df, x='x', y='y', color='type', hover_data=['node'], width=800, height=600)
fig2d.show()

# ===== Cell 9: 3D Graph Layout =====n# Subsample for performance
sub_nodes = labels[:min(SAMPLE_LIMIT, len(labels))]
subG = G.subgraph(sub_nodes).to_undirected()
pos3d = nx.spring_layout(subG, dim=3, seed=42)
edge_x, edge_y, edge_z = [], [], []
for u, v in subG.edges():
    x0,y0,z0 = pos3d[u]; x1,y1,z1 = pos3d[v]
    edge_x += [x0, x1, None]
    edge_y += [y0, y1, None]
    edge_z += [z0, z1, None]

edge_trace = go.Scatter3d(x=edge_x, y=edge_y, z=edge_z, mode='lines', line=dict(width=1, color='grey'), hoverinfo='none')
node_trace = go.Scatter3d(
    x=[pos3d[n][0] for n in pos3d],
    y=[pos3d[n][1] for n in pos3d],
    z=[pos3d[n][2] for n in pos3d],
    mode='markers',
    marker=dict(size=5, symbol='circle'),
    text=[f"{n} ({G.nodes[n]['type']})" for n in pos3d]
)
fig3d = go.Figure([edge_trace, node_trace])
fig3d.update_layout(width=800, height=600, title='3D KG Subgraph')
fig3d.show()

# ===== Cell 10: Extract Graph Features =====ndeg = nx.degree_centrality(G)
clust = nx.clustering(G.to_undirected())
pr = nx.pagerank(G.to_undirected())
feat_df = pd.DataFrame({
    'node': list(deg.keys()),
    'deg_centrality': list(deg.values()),
    'clustering': [clust[n] for n in deg],
    'pagerank': [pr[n] for n in deg]
})
# Merge with event rows
feat_df['event_idx'] = feat_df['node'].str.extract(r'event_(\d+)').astype(float)
merged = df.reset_index().merge(feat_df, left_on='index', right_on='event_idx', how='left')
print("Graph features added to 'merged' DataFrame.")

# ===== Cell 11: Feature Distribution =====nfig_box = px.box(merged, x='Severity', y='deg_centrality', points='all', width=800, height=600,
                 title='Degree Centrality by Severity')
fig_box.show()

print("Pipeline completed successfully.")