# -*- coding: utf-8 -*-
"""distilRoBERTa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14gsIYoZ1NEFRzs2HPoffshbixoWfj-h5
"""

import os
os.environ["WANDB_DISABLED"] = "true"

# Colab will treat this as bash
!pip install \
    torch torchvision torchaudio \
    transformers pandas scikit-learn openpyxl matplotlib evaluate

APACHE_XLSX = '/content/drive/MyDrive/LLM4Sec/apache_logs.csv'
THUNDERBIRD_CSV = '/content/drive/MyDrive/LLM4Sec/ThunderBird1K.csv'

import pandas as pd

# — Apache logs —
df_apache = pd.read_csv(APACHE_XLSX)
# Map your “detected” column to “Anomalous” / “Normal”
df_apache['label'] = df_apache['detected'].map({
    'BAHAYA':      'Anomalous',
    'DICURIGAI':   'Anomalous',
    'AMAN':        'Normal'
})
# Concatenate the fields you want into a single text column
df_apache['log'] = df_apache[['ip','datetime','request','status','referer','browser']]\
                       .astype(str)\
                       .agg(' '.join, axis=1)
df_apache = df_apache[['log','label']].dropna()
df_apache.to_csv('apache_logs.csv', index=False)


# — Thunderbird logs —
df_th = pd.read_csv(THUNDERBIRD_CSV)
df_th['log'] = df_th[['hostname','process_info','log_level','full_message']]\
                  .astype(str)\
                  .agg(' '.join, axis=1)
# If you don’t yet have labels, you can leave as “Unknown” or apply clustering later
df_th['label'] = 'Unknown'
df_th[['log','label']].to_csv('thunderbird_logs_unlabeled.csv', index=False)

print("Apache sample:\n", df_apache.head(), "\n")
print("Thunderbird sample:\n", df_th.head(), "\n")

# Sample 1,000 from each class for balanced data
normal_sample = df_apache[df_apache['label'] == 'Normal'].sample(n=2000, random_state=42)
anomalous_sample = df_apache[df_apache['label'] == 'Anomalous'].sample(n=2000, random_state=42)
df_apache_balanced = pd.concat([normal_sample, anomalous_sample]).sample(frac=1, random_state=42)

# Save this balanced data for training
df_apache_balanced.to_csv('apache_logs_balanced.csv', index=False)

from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
import evaluate
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

def load_and_split(path):
    df = pd.read_csv(path).dropna(subset=["log","label"])
    # Map string labels to ints
    label_map = {"Anomalous":0, "Normal":1}
    df["label"] = df["label"].map(label_map)
    # Stratified split
    train_df, test_df = train_test_split(
        df, test_size=0.25, stratify=df["label"], random_state=42
    )
    return DatasetDict({
        "train": Dataset.from_dict(train_df.to_dict(orient="list")),
        "test":  Dataset.from_dict(test_df .to_dict(orient="list"))
    })

def finetune_distilroberta(csv_path, suffix, num_epochs=3):
    # 1. Load & split
    ds = load_and_split(csv_path)
    # 2. Tokenizer
    tok = AutoTokenizer.from_pretrained("distilroberta-base")
    def tok_fn(x): return tok(x["log"], truncation=True)
    tokenized = ds.map(tok_fn, batched=True)
    # 3. Collator & metrics
    collator = DataCollatorWithPadding(tok)
    accuracy = evaluate.load("accuracy")
    def compute_metrics(eval_pred):
        preds = np.argmax(eval_pred.predictions, axis=1)
        return accuracy.compute(predictions=preds, references=eval_pred.label_ids)
    # 4. Model & Trainer
    model = AutoModelForSequenceClassification.from_pretrained(
        "distilroberta-base",
        num_labels=2,
        id2label={0:"Anomalous",1:"Normal"},
        label2id={"Anomalous":0,"Normal":1}
    )
    args = TrainingArguments(
        output_dir=f"{suffix}_model",
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=num_epochs,
        logging_steps=50,
        learning_rate=2e-5,
        weight_decay=0.01,
        report_to="none"
    )
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized["train"],
        eval_dataset= tokenized["test"],
        tokenizer=tok,
        data_collator=collator,
        compute_metrics=compute_metrics
    )
    trainer.train()
    return model, tok, tokenized["test"]

# For Apache:
model_apache, tok_apache, test_apache = finetune_distilroberta(
    "apache_logs_balanced.csv", "Apache", num_epochs=1
)

# For Thunderbird:
model_thunderbird, tok_thunderbird, test_thunderbird = finetune_distilroberta(
    "thunderbird_logs_unlabeled.csv", "Thunderbird", num_epochs=1
)

import torch
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_auc_score, roc_curve

def evaluate_and_plot(model, tok, ds, name):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device).eval()
    preds, probs, labels = [], [], []
    for ex in ds:
        enc = tok(ex["log"], truncation=True, padding=True, return_tensors="pt", max_length=512)
        enc = {k:v.to(device) for k,v in enc.items()}
        with torch.no_grad():
            out = model(**enc).logits.cpu().numpy()[0]
        pred = out.argmax()
        prob = torch.softmax(torch.tensor(out), dim=0)[1].item()
        preds.append(pred)
        probs.append(prob)
        labels.append(ex["label"])

    print(f"--- {name} Classification Report ---")
    print(classification_report(labels, preds, target_names=["Anomalous","Normal"]))

    fpr, tpr, _ = roc_curve(labels, probs)
    auc = roc_auc_score(labels, probs)
    plt.plot(fpr,tpr, label=f"{name} (AUC={auc:.2f})")

# Plot both on one figure
plt.figure(figsize=(7,6))
evaluate_and_plot(model_apache, tok_apache, test_apache, "Apache")
#evaluate_and_plot(model_thunderbird, tok_thunderbird, test_thunderbird, "Thunderbird")
plt.plot([0,1],[0,1], '--', color='gray')
plt.xlabel("FPR"); plt.ylabel("TPR"); plt.legend()
plt.title("ROC Curves"); plt.grid(); plt.show()

"""TESTING ON APACHE"""

import pandas as pd

# Load your new logs
df_new = pd.read_csv('/content/drive/MyDrive/LLM4Sec/NewLogs.csv')

# Create a single log string per row
df_new['log'] = df_new[['ip','datetime','request','status','referer','browser']].astype(str).agg(' '.join, axis=1)

import torch

# Make sure your model and tokenizer are loaded
# Example: model_myws, tok_myws to model_apache, tok_apache

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_apache.to(device).eval()

results = []
for idx, row in df_new.iterrows():
    enc = tok_apache(row['log'], truncation=True, padding=True, return_tensors="pt", max_length=512)
    enc = {k:v.to(device) for k,v in enc.items()}
    with torch.no_grad():
        logits = model_apache(**enc).logits
        pred = logits.argmax().item()
        label = "Anomalous" if pred == 0 else "Normal"
    results.append(label)
    print(f"Row {idx}: {label}")

df_new['predicted_label'] = results

"""CLASSIFYING THUNDERBIRD LOGS"""

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_apache.to(device).eval()  # Use your already trained model

results = []
for idx, row in df_th.iterrows():
    enc = tok_apache(row['log'], truncation=True, padding=True, return_tensors="pt", max_length=512)
    enc = {k: v.to(device) for k, v in enc.items()}
    with torch.no_grad():
        logits = model_apache(**enc).logits
        pred = logits.argmax().item()
        label = "Anomalous" if pred == 0 else "Normal"
    results.append(label)
    print(f"Row {idx}: {label}")

df_th['predicted_label'] = results

df_th.to_csv('thunderbird_with_predictions.csv', index=False)

df_apache = pd.read_csv('/content/thunderbird_with_predictions.csv')
print(df_apache['predicted_label'].value_counts())

# Making a directory to save model + tokenizer
output_dir = "distilroberta-apache-finetuned"
model_apache.save_pretrained(output_dir)
tok_apache.save_pretrained(output_dir)

#Saving the model in Drive
import shutil
shutil.copytree(output_dir, "/content/drive/MyDrive/LLM4Sec/" + output_dir)