# -*- coding: utf-8 -*-
"""Week3.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cbtGz4inCPRtOd7gE8vh7OkCB19rOUJj
"""

import pandas as pd

# --- 1. Load all datasets ---
logs = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/apache_logs_labelled.csv')
ip_entropy = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/ip_entropy_features.csv')
geolocation = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/geolocation_enriched.csv')
session_stats = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/session_stats.csv')
# (Optional) Load time-based and sequence features if you wish to merge them:
time_based = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/time_based_features.csv')
sequence_patterns = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/sequence_patterns_features.csv')

# --- 2. Merge IP-based features ---
# Add IP entropy metrics
logs = logs.merge(ip_entropy, on='ip', how='left')

# Add geolocation features (pick only unique IP info, drop duplicates for safe join)
geo_cols = ['ip', 'country_x', 'region', 'city', 'asn', 'reverse_dns']
geo_info = geolocation[geo_cols].drop_duplicates('ip')
logs = logs.merge(geo_info, on='ip', how='left')

# --- 3. Merge Session-level features ---
# Take the most recent session per IP (can change to aggregate if you have session_id per row)
session_latest = session_stats.sort_values('end_time').groupby('ip').tail(1)
session_cols = [col for col in session_stats.columns if col not in ['ip', 'start_time', 'end_time']]
logs = logs.merge(session_latest[['ip'] + session_cols], on='ip', how='left')

# --- 4. (Optional) Merge Time-based/Sequence features if needed ---
# If you want to add time-based, you'll need to round/align timestamps for a good join (not shown here)
# logs = logs.merge(time_based, ...)

# --- 5. Save unified DataFrame for analysis and modeling ---
logs.to_csv('logs_features_unified.csv', index=False)

print("Unified feature set shape:", logs.shape)
print(logs.head())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load your merged dataset
logs = pd.read_csv('/content/logs_features_unified.csv')

# Select only numeric columns for correlation
numeric_cols = logs.select_dtypes(include=['number']).columns
corr_matrix = logs[numeric_cols].corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap of Numeric Features (Merged Logs)')
plt.tight_layout()
plt.show()

time_based['datetime'] = pd.to_datetime(time_based['datetime'])
time_based.set_index('datetime', inplace=True)

time_based['request_count_mean_1h'].plot(figsize=(15, 5), title='Request Rate per Hour - Detecting Spikes')
plt.ylabel('Request Count Mean (1h)')
plt.show()

logs.columns

"""Cleaning the Data"""

import pandas as pd

# Load data
session_df = pd.read_csv("/content/drive/MyDrive/LLM4Sec/Week3/session_stats.csv")
entropy_df = pd.read_csv("/content/drive/MyDrive/LLM4Sec/Week3/ip_entropy_features.csv")

# Merge
combined_df = session_df.merge(entropy_df, on="ip", how="left")

# Keep only relevant features
features = ["request_count", "duration_s", "unique_ips", "error_ratio", "req_per_min", "url_entropy", "user_agent_entropy"]
df = combined_df[features]

# Drop rows with missing or non-numeric values
df = df.dropna()
df = df.apply(pd.to_numeric, errors='coerce').dropna()

"""Pairplot"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")
sns.pairplot(df, diag_kind="kde")
plt.suptitle("Pairwise Feature Analysis for Anomaly Detection", y=1.02)
plt.show()

"""Correlation Heatmap"""

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap of Selected Features")
plt.show()

"""Distribution Plots"""

for col in df.columns:
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.show()

"""Prepare data for LSTM"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Scale data
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

# Create sequences
def create_sequences(data, seq_length):
    xs = []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        xs.append(x)
    return np.array(xs)

SEQ_LEN = 10
X = create_sequences(scaled, SEQ_LEN)

"""Build LSTM Model"""

from keras.models import Sequential
from keras.layers import LSTM, Dense, TimeDistributed, RepeatVector

model = Sequential()
model.add(LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))
model.add(RepeatVector(X.shape[1]))  # Repeat output to match time steps
model.add(LSTM(64, return_sequences=True))
model.add(TimeDistributed(Dense(X.shape[2])))  # Predict all time steps
model.compile(optimizer='adam', loss='mse')
model.summary()

"""Train & Detect Anomalies"""

history = model.fit(X, X, epochs=20, batch_size=64, validation_split=0.1)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

X_pred = model.predict(X)
mse = np.mean(np.power(X - X_pred, 2), axis=(1, 2))

sns.histplot(mse, bins=50, kde=True)
plt.title("Reconstruction Error (Anomaly Score Distribution)")
plt.xlabel("MSE Loss")
plt.ylabel("Frequency")
plt.show()

threshold = np.percentile(mse, 95)  # top 5% as anomaly
anomalies = mse > threshold
print(f"Anomalies detected: {np.sum(anomalies)} out of {len(anomalies)} sequences.")

"""Visualization"""

plt.plot(mse)
plt.axhline(threshold, color='red', linestyle='--')
plt.title("Anomaly Score Over Time")
plt.xlabel("Sequence index")
plt.ylabel("Reconstruction Error (MSE)")
plt.show()

"""Correlation with MSE features"""

# Create anomaly_scores_df from MSE
anomaly_scores_df = pd.DataFrame({'anomaly_score': mse})
anomaly_scores_df = anomaly_scores_df.iloc[SEQ_LEN:].reset_index(drop=True)

anomaly_scores_df.to_csv("/content/drive/MyDrive/LLM4Sec/Week3/anomaly_scores.csv", index=False)

# Safely join anomaly scores
combined_df_for_analysis = combined_df.iloc[SEQ_LEN:].reset_index(drop=True)
full_df = combined_df_for_analysis.join(anomaly_scores_df)

# Drop non-numeric columns (like 'ip', timestamps, etc.)
numeric_df = full_df.select_dtypes(include='number')

# Compute correlation only on numeric features
correlation = numeric_df.corr()

# Plot correlation with anomaly score
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(correlation[['anomaly_score']].sort_values(by='anomaly_score', ascending=False), annot=True, cmap='coolwarm')
plt.title("Feature Correlation with Anomaly Score")
plt.show()

!pip install pyngrok

from pyngrok import ngrok
ngrok.kill()  # ✅ Closes all tunnels

from pyngrok import ngrok, conf

# Paste your token here
authtoken = "2ynvpzO9hx7sertKPpra7QaITwE_3oT6hDWpJc2C6bWhp4zSP"
conf.get_default().auth_token = authtoken

!pip install streamlit pyngrok
from pyngrok import ngrok

public_url = ngrok.connect(8501)
print(f"🌐 Open this Streamlit app URL: {public_url}")

!streamlit run "/content/drive/MyDrive/LLM4Sec/Week3/app1.py" &> /dev/null &

df = pd.read_csv("/content/drive/MyDrive/LLM4Sec/Week3/geolocation_enriched.csv")
print("Countries in data:", df['country_x'].unique())
print("Top ASNs:", df['asn'].value_counts().head(10))

# Assuming label column is named 'label'
true_labels = session_df['label'].values  # Replace with actual column name if different

# Drop NA or reset if needed
true_labels = pd.Series(true_labels).dropna().astype(int).values

# Align with LSTM sequences
SEQ_LEN = 10
true_labels_trimmed = true_labels[SEQ_LEN:]

# Predicted labels from LSTM
predicted_labels = (mse > threshold).astype(int)

# ✅ Sanity check
print(f"True: {true_labels_trimmed.shape}, Pred: {predicted_labels.shape}")

# Report
from sklearn.metrics import classification_report
print(classification_report(true_labels_trimmed, predicted_labels, target_names=['Normal', 'Anomaly']))