# -*- coding: utf-8 -*-
"""Week5(Topic Modelling - LDA) features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKf6uK03eIcYe9eBwI7-Yi5moc_qK9XL
"""

from google.colab import drive
drive.mount('/content/drive')

# Check GPU
import torch
print("PyTorch CUDA available?", torch.cuda.is_available())

# Install if needed (Colab often has these, but just in case)
!pip install -U sentence-transformers
!pip install umap-learn

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/LLM4Sec/Week4/BGL_log.csv")
print("Loaded shape:", df.shape)
print(df.head())

"""#Column Types
Numerical - unix_Timestamp, Date

Categorical - Alert_Tag, System, Facility, Severity

Mixed - Location1, Detailed_Timestamp, Location2, Message

1. Preprocessing & Tokenization
"""

import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['clean_msg'] = df['Message'].apply(clean_text)
stop = set(ENGLISH_STOP_WORDS)
df['tokens'] = df['clean_msg'].apply(lambda txt: [w for w in txt.split() if w not in stop])

"""2. N-gram Frequency Analysis"""

from sklearn.feature_extraction.text import CountVectorizer

# Bigrams
vect2 = CountVectorizer(ngram_range=(2,2), min_df=5, stop_words='english')
X2 = vect2.fit_transform(df['clean_msg'])
bigrams = pd.DataFrame({
    'bigram': vect2.get_feature_names_out(),
    'count': X2.toarray().sum(axis=0)
}).sort_values('count', ascending=False).head(20)
print("Top bigrams:\n", bigrams)

# Trigrams
vect3 = CountVectorizer(ngram_range=(3,3), min_df=3, stop_words='english')
X3 = vect3.fit_transform(df['clean_msg'])
trigrams = pd.DataFrame({
    'trigram': vect3.get_feature_names_out(),
    'count': X3.toarray().sum(axis=0)
}).sort_values('count', ascending=False).head(20)
print("Top trigrams:\n", trigrams)

"""3. TF-IDF & Top Terms by Severity"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

tfidf = TfidfVectorizer(stop_words='english', max_features=5000)

for sev in df['Severity'].unique():
    msgs = df.loc[df['Severity']==sev, 'clean_msg']
    X = tfidf.fit_transform(msgs)
    mean_scores = np.asarray(X.mean(axis=0)).flatten()
    top_idx = mean_scores.argsort()[::-1][:10]
    top_terms = [tfidf.get_feature_names_out()[i] for i in top_idx]
    top_scores = mean_scores[top_idx]
    print(f"\nTop TF-IDF terms for {sev}:")
    for term, score in zip(top_terms, top_scores):
        print(f"{term:15} {score:.4f}")

"""4. Topic Modeling with LDA"""

from sklearn.decomposition import LatentDirichletAllocation

uni_vect = CountVectorizer(stop_words='english', max_features=2000)
X_uni = uni_vect.fit_transform(df['clean_msg'])

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X_uni)

def show_topics(model, feature_names, n_top=8):
    for idx, comp in enumerate(model.components_):
        terms = [feature_names[i] for i in comp.argsort()[:-n_top - 1:-1]]
        print(f"Topic {idx}: {', '.join(terms)}")

show_topics(lda, uni_vect.get_feature_names_out())

"""5. Sentence Embeddings on GPU"""

from sentence_transformers import SentenceTransformer

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Embedding device:", device)

embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)
embs = embedder.encode(df['clean_msg'].tolist(), show_progress_bar=True, batch_size=64, device=device)

"""CPU Clustering & Visualization"""

from sklearn.cluster import MiniBatchKMeans
import numpy as np
import umap
import matplotlib.pyplot as plt

# 1. MiniBatchKMeans for clustering (RAM-friendly)
km = MiniBatchKMeans(n_clusters=4, random_state=42, batch_size=1024)
df['cluster'] = km.fit_predict(embs)

# 2. Random subsample for visualization
n_samples = min(2000, len(df))
idx = np.random.choice(len(df), n_samples, replace=False)
embs_sample = embs[idx]
labels_sample = df['cluster'].values[idx]

# 3. UMAP on the sample (CPU is ok)
reducer = umap.UMAP(n_components=2, random_state=42)
emb2d = reducer.fit_transform(embs_sample)

plt.figure(figsize=(8,6))
plt.scatter(emb2d[:,0], emb2d[:,1], c=labels_sample, cmap='tab10', s=8)
plt.title("UMAP Projection of Log Embeddings (subset)")
plt.show()

"""Feature Engineering for Selection"""

# Example: message length, cluster, etc.
df['msg_len'] = df['clean_msg'].apply(len)
# You can add more features: entropy, counts, etc.

import numpy as np

# Example combined feature matrix for selection
features = np.concatenate([
    embs,
    df[['msg_len', 'cluster']].values
], axis=1)

# Example labels (modify for my real target/anomaly column)
labels = (df['Alert_Tag'] != '-').astype(int) if 'Alert_Tag' in df.columns else df['cluster']

"""Feature Selection: Random Forest Importance"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(features, labels)

# Feature names
emb_dim = embs.shape[1]
feature_names = [f'emb_{i}' for i in range(emb_dim)] + ['msg_len', 'cluster']
importances = clf.feature_importances_

feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})
feat_imp = feat_imp.sort_values('importance', ascending=False)
print(feat_imp.head(20))

"""##Conclusion: Exploratory Data Analysis on BGL Logs
###NLP-based Text Analysis:

Frequent Bigrams/Trigrams: The log data is dominated by technical patterns like "generating core", "tlb error", "data tlb", and "error interrupt". These frequent patterns reflect the recurring nature of system and hardware events, making them important signals for normal and abnormal system behavior.

TF-IDF by Severity: Key terms differ notably across severity levels. For example, INFO logs are characterized by “core”, “generating”, and “corrected”; FATAL by “data”, “interrupt”, “tlb”, “storage”; ERROR by “error”, “active”, “asserted”, “latch”, “temperature”; and so on. This highlights that each severity category has distinct term signatures, supporting targeted anomaly detection or filtering.

###Topic Modeling:

Latent Dirichlet Allocation (LDA) surfaced coherent technical topics, with clusters related to errors, registers, interrupts, addresses, and other hardware/software signals. This shows that the log messages contain enough linguistic structure for topic modeling to reveal operational domains or error themes.

###Sentence Embedding & Clustering:

Using transformer embeddings and clustering (MiniBatchKMeans), logs naturally grouped into several dense clusters, as shown by UMAP visualization. A notable result is the presence of a very tight, central cluster—likely representing routine, repeated messages—and several sparser, more distributed groups, likely representing diverse error or anomaly types.

The ability to visualize distinct log clusters can aid in surfacing rare or novel events, outlier detection, and even pre-filtering for further investigation.

###Feature Engineering & Selection:

Random Forest feature importance ranking showed that certain embedding dimensions (e.g., emb_351, emb_37, etc.) carry significant information for distinguishing between normal and anomalous logs.

Engineered features like message length and cluster assignment can further enrich models, but the deep text embeddings dominate in importance—justifying the use of modern NLP for log mining.

###Key Takeaways:
Logs can be meaningfully structured and separated using a combination of NLP-based features and clustering, even without labeled anomalies.

Distinctive patterns in text exist for each severity and can be mined for signatures of failures or threats.

Transformer embeddings, together with lightweight clustering and visualization, allow for scalable, interpretable log analysis even in RAM-limited environments like Colab Free.

Feature selection confirms that deep NLP features are highly effective, but engineered statistical features still add value and should be included in downstream models.

###Recommended:
This analysis pipeline can now be extended for:

- Automated anomaly detection (using cluster distances, rare topic events, or supervised classifiers if labels exist).

- Root cause analysis, by focusing on high-importance terms, clusters, and topic signatures that correspond to incidents.

- Incremental monitoring, to surface emerging log patterns or unknown failure modes in production.

###1. Automated Anomaly Detection
####A. Using Cluster Distance (Unsupervised Outlier Detection)
Detects logs that are far from any cluster centroid (potential anomalies).
"""

import numpy as np

# Distance from each log embedding to its assigned cluster centroid
centroids = km.cluster_centers_
distances = np.linalg.norm(embs - centroids[df['cluster']], axis=1)
df['cluster_dist'] = distances

# Mark top 1% furthest as anomalies
threshold = np.percentile(distances, 99)
df['anomaly_cluster_dist'] = (df['cluster_dist'] > threshold).astype(int)

print(f"Number of cluster-distance-based anomalies: {df['anomaly_cluster_dist'].sum()}")

"""####B. Using Rare Topic Events
Flag logs whose most-likely topic is a rare topic (e.g., a topic assigned to <5% of messages).
"""

topic_probs = lda.transform(X_uni)
top_topic = topic_probs.argmax(axis=1)
df['top_topic'] = top_topic

# Find rare topics
topic_counts = np.bincount(top_topic)
rare_topics = np.where(topic_counts < 0.05 * len(df))[0]
df['anomaly_rare_topic'] = df['top_topic'].isin(rare_topics).astype(int)

print("Rare topic indices:", rare_topics)
print(f"Number of rare-topic-based anomalies: {df['anomaly_rare_topic'].sum()}")

"""####C. Using Supervised Classifier (since, labels exist)
Train a classifier to predict anomalies (For 'Alert_Tag' as label).
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

if 'Alert_Tag' in df.columns:
    y = (df['Alert_Tag'] != '-').astype(int)  # Assuming '-' is normal, others are anomalous
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(features, y)
    df['pred_supervised'] = clf.predict(features)
    print(classification_report(y, df['pred_supervised']))

"""###2. Root Cause Analysis
Surface the most important terms/topics for each detected anomaly.
"""

# For a flagged anomaly, print top TF-IDF terms and topic words
def print_root_cause(idx):
    print("\nLog message:", df.loc[idx, 'Message'])
    print("Severity:", df.loc[idx, 'Severity'])
    print("Cluster:", df.loc[idx, 'cluster'])
    print("Cluster distance:", df.loc[idx, 'cluster_dist'])
    print("Assigned topic:", df.loc[idx, 'top_topic'])

    # Show top words in the topic
    topic_idx = df.loc[idx, 'top_topic']
    topic_words = np.array(uni_vect.get_feature_names_out())[lda.components_[topic_idx].argsort()[-10:][::-1]]
    print("Top topic words:", topic_words)

# Example: Print for the first detected cluster anomaly
anomaly_idx = df[df['anomaly_cluster_dist'] == 1].index[0]
print_root_cause(anomaly_idx)

"""###3. Incremental Monitoring
Detect new patterns emerging over time (new clusters or topics appearing, spikes in cluster distance, etc).
"""

import matplotlib.pyplot as plt

# Time-based rolling anomaly count (assuming Detailed_Timestamp is datetime)
df['Detailed_Timestamp'] = pd.to_datetime(
    df['Detailed_Timestamp'],
    format='%Y-%m-%d-%H.%M.%S.%f',
    errors='coerce'  # sets unparseable to NaT instead of crashing
)

df = df.sort_values('Detailed_Timestamp')

df['anomaly_any'] = df[['anomaly_cluster_dist', 'anomaly_rare_topic']].max(axis=1)
rolling_window = '1H'  # one hour window, adjust as needed

df.set_index('Detailed_Timestamp', inplace=True)
rolling_anomaly_count = df['anomaly_any'].rolling(rolling_window).sum()

plt.figure(figsize=(12,4))
rolling_anomaly_count.plot()
plt.title("Rolling Anomaly Count Over Time")
plt.xlabel("Time")
plt.ylabel("Number of Anomalies (per hour)")
plt.show()

# Optional: Surface new clusters/topics as they appear
new_cluster_alert = df['cluster'].diff().abs() > 0
if new_cluster_alert.any():
    print("New cluster assignments detected at times:")
    print(df.index[new_cluster_alert])

"""| Operation                  | Approach / What Was Done                                        | Key Results / Insights                                                                            | Value Added                                                     |
| -------------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Anomaly Detection**      | Cluster distance outlier detection<br>Supervised classification | 9,214 anomalies flagged (1% furthest in cluster space)<br>0 rare topics<br>Classifier: perfect F1 | Unusual events easily surfaced; pipeline is robust and accurate |
| **Root Cause Analysis**    | Topic modeling and feature surfacing for anomalies              | Example: Hardware issue detected via key topic words and context                                  | Rapid triage; context-rich insights for incident response       |
| **Incremental Monitoring** | Rolling anomaly count (hourly)<br>Detection of new clusters     | Clear spikes signal abnormal system activity<br>100k+ new cluster events over time                | Real-time detection of emerging issues, pattern shifts          |

**Takeaway:**

The combination of NLP clustering, anomaly scoring, topic modeling, and incremental monitoring enables reliable detection, fast diagnosis, and proactive awareness of both known and unknown log events in your system.

| **Alert Tag** | **Facility** | **Severity**  | **Common Issue/Threat Type**      | **Keywords / Typical Messages**     | **Example Impact**         |
| ------------- | ------------ | ------------- | --------------------------------- | ----------------------------------- | -------------------------- |
| APPALLOC      | APP          | ERROR, FATAL  | Application allocation failures   | "allocation failed", "memory error" | Service interruption       |
| APPCHILD      | APP          | ERROR, INFO   | Child process errors              | "child exited", "child failed"      | Partial app failure        |
| APPREAD       | APP          | ERROR, FATAL  | Application read/data errors      | "failed to read", "socket error"    | Data loss, incomplete ops  |
| KERNDTLB      | KERNEL       | SEVERE, ERROR | Kernel DTLB (TLB) hardware faults | "tlb error", "parity", "interrupt"  | System instability, crash  |
| KERNMC        | KERNEL       | SEVERE, FATAL | Kernel Machine Check, ECC/parity  | "machine check", "ecc error"        | Hardware failure, downtime |
| KERNRTSP      | KERNEL       | WARNING, INFO | Kernel real-time support events   | "rtsp event", "timer expired"       | Performance degradation    |
| KERNSTOR      | KERNEL       | ERROR, FATAL  | Kernel storage/IO errors          | "storage error", "disk failure"     | Data corruption/loss       |
| (–)           | Any          | INFO          | Routine/system message (benign)   | "operation successful", "core"      | Normal system activity     |

#### Code to Find Keyword Overlap Across Alert Tags
"""

#Keyword Overlap Across Alert Tags

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Number of top keywords per tag
N = 15
tfidf = TfidfVectorizer(stop_words='english', max_features=3000)

# Collect top N keywords for each Alert_Tag
tag_keywords = {}
for tag in df['Alert_Tag'].unique():
    msgs = df[df['Alert_Tag'] == tag]['clean_msg']
    if len(msgs) < 5:
        continue  # skip very small groups
    X = tfidf.fit_transform(msgs)
    mean_scores = np.asarray(X.mean(axis=0)).flatten()
    top_idx = mean_scores.argsort()[::-1][:N]
    words = [tfidf.get_feature_names_out()[i] for i in top_idx]
    tag_keywords[tag] = set(words)

# Build overlap matrix
tags = list(tag_keywords.keys())
overlap = np.zeros((len(tags), len(tags)), dtype=int)
for i, t1 in enumerate(tags):
    for j, t2 in enumerate(tags):
        overlap[i, j] = len(tag_keywords[t1] & tag_keywords[t2])

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(overlap, annot=True, fmt='d',
            xticklabels=tags, yticklabels=tags,
            cmap='Blues')
plt.title(f'Keyword Overlap (Top {N}) between Alert Tags')
plt.xlabel('Alert Tag')
plt.ylabel('Alert Tag')
plt.tight_layout()
plt.show()

"""####Further Analysis & Visualization"""

# 1. Venn Diagram for two example tags
!pip install matplotlib-venn

from matplotlib_venn import venn2

# pick two tags to compare
tag1, tag2 = tags[:2]
set1, set2 = tag_keywords[tag1], tag_keywords[tag2]

plt.figure(figsize=(6, 4))
venn2([set1, set2], set_labels=(tag1, tag2))
plt.title(f'Venn Diagram: {tag1} vs {tag2}')
plt.show()

# 2. WordClouds for each tag
#!pip install wordcloud

from wordcloud import WordCloud

for tag, words in tag_keywords.items():
    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))
    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud for {tag}')
    plt.show()

"""###SHAP Explainability

"""

!pip install shap

import shap

explainer = shap.TreeExplainer(clf)
shap_values = explainer.shap_values(features)

# If binary classifier, use shap_values[1] for the positive class
values = shap_values[1] if isinstance(shap_values, list) else shap_values

# Summary plot (sample for speed if dataset is large)
shap.summary_plot(values, features, feature_names=feature_names, max_display=20)