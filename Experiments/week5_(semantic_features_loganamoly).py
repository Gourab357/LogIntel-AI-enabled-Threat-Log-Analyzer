# -*- coding: utf-8 -*-
"""Week5 (Semantic Features - logAnamoly).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YASAboTXkJyGsqEbidttVGFkopPzEKdj
"""

# 1Ô∏è‚É£ Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# 2Ô∏è‚É£ Load raw CSV (keep all date columns as strings for custom parsing)
file_path = '/content/drive/MyDrive/LLM4Sec/Week4/BGL_log_train.csv'
dtype = {
    'Alert_Tag': 'category',
    'Unix_Timestamp': 'int64',
    'Date': 'string',
    'Detailed_Timestamp': 'string',
    'Location1': 'string',
    'Location2': 'string',
    'System': 'category',
    'Facility': 'category',
    'Severity': 'category',
    'Message': 'string'
}
df = pd.read_csv(file_path, dtype=dtype, low_memory=False)

# 3Ô∏è‚É£ Explicitly parse your date strings with exact formats
df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d')
df['Detailed_Timestamp'] = pd.to_datetime(
    df['Detailed_Timestamp'],
    format='%Y-%m-%d-%H.%M.%S.%f'
)

# 4Ô∏è‚É£ Convert Unix ‚Üí datetime
df['ts_unix'] = pd.to_datetime(df['Unix_Timestamp'], unit='s')

# 5Ô∏è‚É£ Extract Location1 parts via regex
loc_pattern = (
    r'^(?P<rack>[^-]+)-'
    r'(?P<machine>[^-]+)-'
    r'(?P<node>[^-]+)-'
    r'(?P<interface_type>[^:]+):'
    r'(?P<interface_id>[^-]+)-'
    r'(?P<unit>.+)$'
)
df = pd.concat([df, df['Location1'].str.extract(loc_pattern)], axis=1)

# 6Ô∏è‚É£ Sort by time for all subsequent time‚Äêbased features
df = df.sort_values('ts_unix').reset_index(drop=True)

# 7Ô∏è‚É£ Time‚Äêbased features
ts = df['ts_unix']
df['hour']    = ts.dt.hour
df['weekday'] = ts.dt.weekday   # Monday=0
df['month']   = ts.dt.month
df['day']     = ts.dt.day
df['delta_s'] = ts.diff().dt.total_seconds().fillna(0)

# 8Ô∏è‚É£ Rolling count per node over the past 1 hour (`'1h'`, lowercase)
#    We compute via a merge of the groupby‚Äêrolling result back on (node, ts_unix)

rolling_node = (
    df
    .set_index('ts_unix')
    .groupby('node')['node']
    .rolling('1h')
    .count()
    .reset_index(name='events_1h_node')
)
# now merge to align one‚Äêto‚Äêone
df = df.reset_index(drop=True).merge(
    rolling_node,
    on=['node', 'ts_unix'],
    how='left'
)

# 9Ô∏è‚É£ Message‚Äêbased features
df['msg_len']   = df['Message'].str.len()
df['word_count'] = df['Message'].str.split().str.len()

#  üîü Prepare a transformer to vectorize everything
numeric_features = [
    'hour','weekday','month','day','delta_s',
    'events_1h_node','msg_len','word_count'
]
categorical_feats = [
    'System','Facility','Severity',
    'rack','machine','node','interface_type'
]
text_feature = 'Message'

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer

# ‚Ä¶ assume numeric_features, categorical_feats, text_feature are defined ‚Ä¶

# 1Ô∏è‚É£ Numeric pipeline
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler',  StandardScaler())
])

# 2Ô∏è‚É£ Categorical pipeline
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot',  OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# 3Ô∏è‚É£ Text pipeline: fill NAs with empty string, then TF-IDF
text_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='')),
    ('tfidf',   TfidfVectorizer(
                   max_features=1000,
                   ngram_range=(1,2),
                   stop_words='english'
               ))
])

# 4Ô∏è‚É£ Combine into ColumnTransformer
preprocessor = ColumnTransformer([
    ('num', num_pipeline,   numeric_features),
    ('cat', cat_pipeline,   categorical_feats),
    ('txt', text_pipeline,  text_feature),
], remainder='drop')

# Ensure 'missing' is a valid category before filling NAs
for col in categorical_feats:
    if pd.api.types.is_categorical_dtype(df[col]):
        df[col] = df[col].cat.add_categories('missing')
    df[col] = df[col].fillna('missing')

# For the text column, just replace NAs with an empty string
df[text_feature] = df[text_feature].fillna('')

# Now this will work without errors:
X = preprocessor.fit_transform(df)

print("DataFrame shape:", df.shape)
print("Feature matrix X shape:", X.shape)

