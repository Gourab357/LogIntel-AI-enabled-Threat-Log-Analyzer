# -*- coding: utf-8 -*-
"""Week 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0ZxKutc70SFxu42lqQy7KpZgVdKyGGp
"""

# ─── Cell 1: Install dependencies (run once) ──────────────────────────────────
!pip install pandas numpy matplotlib seaborn scikit-learn umap-learn xmltodict

# ─── Cell 10: Install embedding & HMM libraries ─────────────────────────────
!pip install sentence-transformers hmmlearn

from google.colab import drive
drive.mount('/content/drive')

# ─── Cell 2: Imports ─────────────────────────────────────────────────────────
import pandas as pd
import numpy as np
import re
from collections import Counter

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
import umap  # optional, if you prefer UMAP over t-SNE
import xmltodict

# ─── Cell 3: Utility functions ────────────────────────────────────────────────
def clean_text(s):
    """Lowercase, collapse whitespace."""
    if pd.isna(s): return ""
    s = str(s).lower()
    return re.sub(r"\s+", " ", s).strip()

def apply_command_line_rules(cmd):
    """Mask IPs, URLs, UUIDs, numbers, etc., per ConceptUML."""
    if pd.isna(cmd): return ""
    rules = {
        r"[,]": "",
        r"[ ]{2,}": " ",
        r'"(.+?)"': r"\1",
        r"\'(.+?)\'": r"\1",
        r"(-[a-z]{1,}) \d+": r"\1 num",
        r"(?:(?:https?://)|www\.)\S+": "url",
        r"\S+@\S+\.\S+": "email",
        r"(?:(?:25[0-5]|(?:2[0-4]|1?\d)\d)\.){3}(?:25[0-5]|(?:2[0-4]|1?\d)\d)": "ip",
        r"[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}": "uuid",
    }
    s = str(cmd).lower()
    for patt, repl in rules.items():
        s = re.sub(patt, repl, s)
    return re.sub(r"\s+", " ", s).strip()

def create_event_windows(df, seq_len=10):
    """Non-overlapping windows of length seq_len from df['Event']."""
    ev = df['Event'].tolist()
    wins = []
    for i in range(0, len(ev), seq_len):
        chunk = ev[i:i+seq_len]
        if len(chunk)==seq_len:
            wins.append(" ||| ".join(chunk))
    return pd.DataFrame({"Window": wins})

# ─── Cell 4: Load BGL logs, CAPEC & MITRE tokens, and CWE XML ───────────────
import xmltodict

# Adjust paths to your Drive mount as needed
LOG_PATH   = '/content/drive/MyDrive/LLM4Sec/Week4/BGL_log_train.csv'
CAPEC_CSV  = '/content/drive/MyDrive/LLM4Sec/Week 7/CapecTokens_V5.csv'
MITRE_CSV  = '/content/drive/MyDrive/LLM4Sec/Week 7/MitreTechniquesTokens_V5.csv'
CWE_XML    = '/content/drive/MyDrive/LLM4Sec/Week 7/cwec_v4.17.xml'

# 1) BGL logs
logs  = pd.read_csv(LOG_PATH, low_memory=False)

# 2) Pre-tokenized knowledge bases
capec = pd.read_csv(CAPEC_CSV, encoding='ISO-8859-1')
mitre = pd.read_csv(MITRE_CSV, encoding='ISO-8859-1')

# 3) CWE XML parsing (robust to single vs. list entries)
with open(CWE_XML, 'rb') as f:
    cwe_dict = xmltodict.parse(f)

# Navigate into the <Weakness> entries
weaks = cwe_dict.get('Weakness_Catalog', {}) \
               .get('Weaknesses', {}) \
               .get('Weakness', [])

# xmltodict returns dict if only one entry; wrap into list
if isinstance(weaks, dict):
    weaks = [weaks]

records = []
for w in weaks:
    cwe_id = w.get('@ID') or w.get('ID') or ""
    name   = w.get('Name', "")
    # Description may be nested or a string
    desc_node = w.get('Description', {})
    if isinstance(desc_node, dict):
        desc = desc_node.get('Description_Text') \
             or desc_node.get('Description') \
             or ""
    elif isinstance(desc_node, str):
        desc = desc_node
    else:
        desc = ""
    records.append({
        'cwe_id':   cwe_id,
        'name':     name,
        'desc_raw': desc
    })

cwe = pd.DataFrame(records)

print("Shapes → Logs:", logs.shape,
      "CAPEC:", capec.shape,
      "MITRE:", mitre.shape,
      "CWE:",   cwe.shape)
display(cwe.head())

# ─── Cell 5: Preprocess & build Event strings (robust) ───────────────────────
# 1) Show which columns we actually have
print("Log columns:", logs.columns.tolist())

# 2) Clean the main text column
if 'Message' in logs.columns:
    logs['clean_msg'] = logs['Message'].apply(clean_text)
else:
    text_col = logs.select_dtypes(include='object').columns[0]
    print(f"No 'Message' column—using '{text_col}'")
    logs['clean_msg'] = logs[text_col].apply(clean_text)

# 3) Mask IP/URL/UUIDs etc. on the cleaned text
logs['proc_msg'] = logs['clean_msg'].apply(apply_command_line_rules)

# 4) Build Event by concatenating available fields
event_cols = [c for c in ['System','Facility','Severity'] if c in logs.columns]
if event_cols:
    # row-wise join, then append proc_msg
    logs['Event'] = logs[event_cols].astype(str).agg(' :: '.join, axis=1) \
                    + " :: " + logs['proc_msg']
else:
    logs['Event'] = logs['proc_msg']

# 5) Preview
display(logs[['clean_msg','proc_msg','Event']].head())

# ─── Cell 6: Generate non-overlapping event windows ─────────────────────────
SEQ_LEN = 10  # tweak as desired
windows = create_event_windows(logs, seq_len=SEQ_LEN)
print(f"Generated {len(windows)} windows (seq_len={SEQ_LEN})")
windows.head()

# ─── Cell 7: TF-IDF vectorization of log windows & CWE descriptions ─────────
# Log windows TF-IDF (unchanged)
tfidf_win = TfidfVectorizer(max_features=500, ngram_range=(1,2))
X_win = tfidf_win.fit_transform(windows['Window'])
print("Log-Window TF-IDF shape:", X_win.shape)

# CWE descriptions TF-IDF (ensure desc_clean exists)
if 'desc_clean' not in cwe.columns:
    cwe['desc_clean'] = cwe['desc_raw'].apply(clean_text)

tfidf_cwe = TfidfVectorizer(max_features=200, ngram_range=(1,2))
X_cwe = tfidf_cwe.fit_transform(cwe['desc_clean'])
print("CWE TF-IDF shape:", X_cwe.shape)

# ─── Cell 8: Visualization – Window token counts ─────────────────────────────
windows['tok_count'] = windows['Window'].str.split().apply(len)
plt.figure(figsize=(8,4))
sns.histplot(windows['tok_count'], bins=30, kde=True)
plt.title("Tokens per Log-Window")
plt.xlabel("Token Count")
plt.show()

# ─── Cell 9: Visualization – t-SNE of TF-IDF Windows ────────────────────────
# Sample for speed
sample_idx = np.random.choice(X_win.shape[0], size=min(500, X_win.shape[0]), replace=False)
X_samp = X_win[sample_idx].toarray()

tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')
emb = tsne.fit_transform(X_samp)

plt.figure(figsize=(6,6))
plt.scatter(emb[:,0], emb[:,1], s=8, alpha=0.6)
plt.title("t-SNE of Log-Window TF-IDF")
plt.xlabel("TSNE-1"); plt.ylabel("TSNE-2")
plt.show()



# ─── Cell 11: Sentence-BERT Embeddings + Scaling ────────────────────────────
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import MinMaxScaler

# 1) Load model
model = SentenceTransformer('bert-base-nli-mean-tokens')

# 2) Encode each window
window_texts = windows['Window'].tolist()
emb = model.encode(window_texts, show_progress_bar=True)
print("Raw embeddings shape:", emb.shape)

# 3) Min-max scale to [0,1] for NMF compatibility
scaler = MinMaxScaler()
emb_scaled = scaler.fit_transform(emb)
print("Scaled embeddings shape:", emb_scaled.shape)

# ─── Cell 12: NMF Concept Learning ───────────────────────────────────────────
from sklearn.decomposition import NMF

n_topics = 10   # adjust as desired
nmf = NMF(n_components=n_topics, random_state=42)

# Fit & transform
W = nmf.fit_transform(emb_scaled)
H = nmf.components_
print(f"NMF concept matrix W shape: {W.shape}, H shape: {H.shape}")

# ─── Cell 13: HMM Clustering + Visualization ─────────────────────────────────
from hmmlearn.hmm import GaussianHMM
from sklearn.manifold import TSNE

# 1) Select best HMM by log-likelihood over n_states=4…8
best_score, best_model = float('-inf'), None
for n in range(4,9):
    m = GaussianHMM(n_components=n, covariance_type='tied', random_state=42)
    m.fit(W)
    score = m.score(W)
    if score > best_score:
        best_score, best_model = score, m

print("Best HMM states:", best_model.n_components, "Log-Lik:", best_score)

# 2) Predict clusters
clusters = best_model.predict(W)
windows['cluster'] = clusters

# 3) Plot cluster distribution
plt.figure(figsize=(6,3))
sns.countplot(x=clusters, palette='tab10')
plt.title("HMM Cluster Counts")
plt.xlabel("Cluster ID")
plt.show()

# 4) t-SNE on NMF space colored by cluster
tsne2 = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')
nmf_tsne = tsne2.fit_transform(W)

plt.figure(figsize=(6,6))
plt.scatter(nmf_tsne[:,0], nmf_tsne[:,1], c=clusters, cmap='tab10', s=8, alpha=0.7)
plt.title("t-SNE of NMF Concepts (colored by HMM cluster)")
plt.xlabel("TSNE-1"); plt.ylabel("TSNE-2")
plt.show()

# ─── Enhanced HMM Clustering with Visualization ───

from hmmlearn.hmm import GaussianHMM
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import umap

# ─── 1. Standardize the NMF Concept Matrix (W) ───
scaler = StandardScaler()
W_scaled = scaler.fit_transform(W)

# ─── 2. Select Best HMM using BIC ───
def compute_bic(model, X):
    n_params = model.n_components ** 2 + 2 * model.n_components * X.shape[1] - 1
    return n_params * np.log(X.shape[0]) - 2 * model.score(X)

best_score, best_model = float('-inf'), None
bic_scores = []

for n in range(4, 9):
    model = GaussianHMM(n_components=n, covariance_type='tied', random_state=42)
    model.fit(W_scaled)
    bic = -compute_bic(model, W_scaled)
    bic_scores.append(bic)
    if bic > best_score:
        best_score = bic
        best_model = model

print(f"Best HMM #States: {best_model.n_components}, BIC Score: {best_score:.2f}")

# ─── 3. Predict Clusters ───
clusters = best_model.predict(W_scaled)
windows['cluster'] = clusters

# ─── 4. Plot HMM Cluster Counts ───
plt.figure(figsize=(6,3))
sns.countplot(x=clusters, palette='tab10')
plt.title("HMM Cluster Counts")
plt.xlabel("Cluster ID")
plt.show()

# ─── 5. Plot t-SNE of NMF Space Colored by Cluster ───
tsne2 = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')
nmf_tsne = tsne2.fit_transform(W_scaled)

plt.figure(figsize=(6,6))
plt.scatter(nmf_tsne[:,0], nmf_tsne[:,1], c=clusters, cmap='tab10', s=8, alpha=0.7)
plt.title("t-SNE of NMF Concepts (colored by HMM Cluster)")
plt.xlabel("TSNE-1"); plt.ylabel("TSNE-2")
plt.show()

# ─── 6. Optional: UMAP Visualization ───
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
nmf_umap = reducer.fit_transform(W_scaled)

plt.figure(figsize=(6,6))
plt.scatter(nmf_umap[:,0], nmf_umap[:,1], c=clusters, cmap='tab10', s=8, alpha=0.7)
plt.title("UMAP of NMF Concepts (colored by HMM Cluster)")
plt.xlabel("UMAP-1"); plt.ylabel("UMAP-2")
plt.show()

# ─── 7. Plot Transition Matrix ───
plt.figure(figsize=(5,4))
sns.heatmap(best_model.transmat_, annot=True, fmt=".2f", cmap="Blues")
plt.title("HMM State Transition Matrix")
plt.xlabel("To State")
plt.ylabel("From State")
plt.show()

# ─── 8. Optional: Emission Means Visualization ───
if hasattr(best_model, "means_"):
    plt.figure(figsize=(10,4))
    for i, mean in enumerate(best_model.means_):
        plt.plot(mean, label=f'Cluster {i}')
    plt.title("HMM Emission Means per Cluster")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ─── 9. Evaluate Cluster Quality ───
sil_score = silhouette_score(W_scaled, clusters)
print(f"Silhouette Score: {sil_score:.4f}")

# ─── Cell 14: Embed & Concept-Transform Knowledge Bases (with clipping) ────
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 1) Detect the correct text columns
capec_text_col = ('Description' if 'Description' in capec.columns else
                  'description' if 'description' in capec.columns else
                  capec.columns[-1])
mitre_text_col = ('description' if 'description' in mitre.columns else
                  'Description' if 'Description' in mitre.columns else
                  mitre.columns[-1])

print(f"Using CAPEC text column: '{capec_text_col}'")
print(f"Using MITRE text column: '{mitre_text_col}'")

# 2) Prepare cleaned texts
capec_texts = capec[capec_text_col].astype(str).apply(clean_text).tolist()
mitre_texts = mitre[mitre_text_col].astype(str).apply(clean_text).tolist()
cwe_texts   = cwe['desc_clean'].tolist()

# 3) Embed each corpus with a fresh embedder
embedder = SentenceTransformer('bert-base-nli-mean-tokens')
capec_emb = embedder.encode(capec_texts, show_progress_bar=True)
mitre_emb = embedder.encode(mitre_texts, show_progress_bar=True)
cwe_emb   = embedder.encode(cwe_texts,   show_progress_bar=True)

# 4) Re-scale based on your window embeddings (emb from Cell 11)
scaler_win = MinMaxScaler().fit(emb)
capec_scaled = scaler_win.transform(capec_emb)
mitre_scaled = scaler_win.transform(mitre_emb)
cwe_scaled   = scaler_win.transform(cwe_emb)

# 5) Clip to [0,1] to remove negatives/out-of-range values
capec_scaled = np.clip(capec_scaled, 0.0, 1.0)
mitre_scaled = np.clip(mitre_scaled, 0.0, 1.0)
cwe_scaled   = np.clip(cwe_scaled,   0.0, 1.0)

# 6) Transform into your existing NMF concept space
capec_concepts = nmf.transform(capec_scaled)
mitre_concepts = nmf.transform(mitre_scaled)
cwe_concepts   = nmf.transform(cwe_scaled)

print("Concept shapes → CAPEC:", capec_concepts.shape,
      "MITRE:", mitre_concepts.shape,
      "CWE:",   cwe_concepts.shape)

# ─── Cell 15: Cluster-Centroid Similarity Scoring ───────────────────────────
from sklearn.metrics.pairwise import cosine_similarity

results = []
for cid in np.unique(clusters):
    # 1) Centroid of cluster in NMF space
    idx = np.where(clusters == cid)[0]
    centroid = W[idx].mean(axis=0).reshape(1, -1)
    # 2) Mean cosine to each KB
    sim_cap = cosine_similarity(centroid, capec_concepts).mean()
    sim_mit = cosine_similarity(centroid, mitre_concepts).mean()
    sim_cwe = cosine_similarity(centroid, cwe_concepts).mean()
    combined = np.mean([sim_cap, sim_mit, sim_cwe])
    results.append((cid, sim_cap, sim_mit, sim_cwe, combined))

# 3) Tabulate & sort
res_df = pd.DataFrame(results, columns=['cluster','capec_sim','mitre_sim','cwe_sim','combined'])
res_df = res_df.sort_values('combined', ascending=False).reset_index(drop=True)
display(res_df)

# 4) Flag top cluster
top = res_df.iloc[0]
print(f"Most suspicious cluster = {top.cluster} (combined={top.combined:.3f})")

# ─── Cell 16: (Optional) Word-Clouds of TF-IDF NMF Topics ───────────────────
!pip install wordcloud

from wordcloud import WordCloud

# 1) Perform an NMF on the TF-IDF windows for interpretability
from sklearn.decomposition import NMF
nmf_tfidf_vis = NMF(n_components=n_topics, random_state=42)
W_tfidf_vis  = nmf_tfidf_vis.fit_transform(X_win.toarray())
H_tfidf_vis  = nmf_tfidf_vis.components_

# 2) Generate word-clouds from the top 20 terms per topic
terms = tfidf_win.get_feature_names_out()
for topic_idx, topic_weights in enumerate(H_tfidf_vis):
    # get indices of the top 20 weights
    top_terms = topic_weights.argsort()[-20:]
    freqs = { terms[i]: float(topic_weights[i]) for i in top_terms }

    wc = WordCloud(width=400, height=200).generate_from_frequencies(freqs)
    plt.figure(figsize=(6,3))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"TF-IDF Topic #{topic_idx}")
    plt.show()

# ─── Cell 17: Save Models to Google Drive ───────────────────────────────────
import os
import pickle
from sentence_transformers import SentenceTransformer

# Directory in your Drive
SAVE_DIR = '/content/drive/MyDrive/LLM4Sec/models'
os.makedirs(SAVE_DIR, exist_ok=True)

# 1) Persist NMF, HMM, TF-IDF vectorizer, and scaler
with open(os.path.join(SAVE_DIR, 'nmf_model.pkl'),  'wb') as f:
    pickle.dump(nmf, f)
with open(os.path.join(SAVE_DIR, 'hmm_model.pkl'),  'wb') as f:
    pickle.dump(best_model, f)
with open(os.path.join(SAVE_DIR, 'tfidf_win.pkl'),  'wb') as f:
    pickle.dump(tfidf_win, f)
with open(os.path.join(SAVE_DIR, 'scaler_win.pkl'), 'wb') as f:
    pickle.dump(scaler_win, f)

# 2) Save the Sentence-BERT model
sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')
sbert_model.save(os.path.join(SAVE_DIR, 'sbert_model'))

print(f"All models saved under {SAVE_DIR}/")